We are going to have a dag in apache airflow which is fectching data from the api intermittently (stopping for a short time and then starting again several times) and the apache airflow with our configuration is going to be running on postgres SQL. The data fectch is going to be stremed into Kafka which is sitting on Apache zookeeper and the zookeeper is the manager to manage all the multiple brokers(which have multiple topics in it)  of kafka then the data inside the kafka broker (which have multiple topics in it) will be visualized in our control center . The control center serves as a  UI where we can see what is going on in our kafka broker , the number of messages that are coming to different topics
. While the schema registory it provides us several (or seven , have to check ) layer for the metadata , the schema registory is a restful interface for storing and retrieving average schema  which is particularly useful when the kafka streams is being visualized , so we can understand the schema of the records of all the data that is coming into kafka.
The data we get from the kafka will be streamed with the Apache spark . We have a master architecture being set up on Apache Spark . When a job is submitted to the master , then the master will decide which of the worker takes up the job and run the data and  run the task on that data. One task is to be run , the task in this case will be to stream data into Cassandra . there will be the listener which is getting data from kafka to spark and then stream that data to Cassandra

All this architecture will be going to run on the docker container . This docker container will be have the custom image made by us with all the necessary libraries and other things we need in project